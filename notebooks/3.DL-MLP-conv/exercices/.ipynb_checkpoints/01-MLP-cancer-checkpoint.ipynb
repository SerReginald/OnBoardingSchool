{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de pathologies cancéreuses à partie de paramètres physiologiques en utilisant un perceptron multi-couches\n",
    "\n",
    "#### Françoise Bouvet (IJCLab, CNRS)  francoise.bouvet@ijclab.in2p3.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "2. [Préparation des données](#Préparation-des-données)\n",
    "3. [Structure du réseau](#Structure-du-réseau)\n",
    "4. [Apprentissage](#Apprentissage)\n",
    "5. [Evaluation](#Evaluation)\n",
    "6. [Exercice](#Exercice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "L'objectif de cet exercice est de créer un MLP pour prédire la présence ou l'absence de cancer du sein à partir de 9 paramètres physiologiques.   \n",
    "Les données proviennent de la plateforme [kaggle](https://www.kaggle.com/).  \n",
    "Le code est écrit avec la librairie open source [keras](https://keras.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structure des données\n",
    " \n",
    "L'ensemble de données est un fichier \".csv\" qui contient 10 colonnes de nombres réels : \n",
    "- Age (years)\n",
    "- BMI (kg/m2)\n",
    "- Glucose (mg/dL)\n",
    "- Insulin (µU/mL)\n",
    "- HOMA\n",
    "- Leptin (ng/mL)\n",
    "- Adiponectin (µg/mL)\n",
    "- Resistin (ng/mL)\n",
    "- MCP-1(pg/dL)  \n",
    "\n",
    "La dernière colonne contient \"1\" pour l'absence de cancer et \"2\" pour la présence de cancer.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prétraitement des données\n",
    "- Lire les données (read_csv() de la librairie pandas)\n",
    "- Encoder l'étiquette de classe\n",
    "- Normaliser chaque paramètre sur [0-1]\n",
    "- Mélanger les données (shuffle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_read = pd.read_csv('../datasets/data_cancer.csv')\n",
    "print(data_read.head())\n",
    "print(data_read.columns.values.tolist())\n",
    "print(f\"Le fichier de données contient {data_read.shape[0]} échantillons et {data_read.shape[1]} \"\"paramètres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séparation de la dernière colonne (contient l'étiquette) et étiquetage en 0 et 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data in variable data (copy)\n",
    "data = data_read.copy()\n",
    "\n",
    "# Select last column as label (pop)\n",
    "data_labels = data.pop('Classification')\n",
    "\n",
    "# Convert into numpy array and substract 1 to have class 0 and class 1 \n",
    "data_labels =...\n",
    "\n",
    "print(np.shape(data_labels))\n",
    "print(data.columns.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation des données d'entrée\n",
    "Alternative : les paramètres peuvent également être normalisés en ajoutant une couche de normalisation au réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise features\n",
    "#for column in data.columns:\n",
    "#    data[column] = (data[column] - data[column].min()) / (data[column].max() - data[column].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversion des données d'entrée en tableau :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into numpy array \n",
    "data_features = ...\n",
    "print(np.shape(data_features), np.mean(data_features, axis=0), np.min(data_features), np.max(data_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mélange des échantillons (shuffle)\n",
    "Génère un tableau d'indices qui est ensuite appliqué à la fois aux paramètres (input) et aux étiquettes (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle index\n",
    "ind = np.arange(0, np.shape(data_features)[0])\n",
    "np.random.shuffle(ind)\n",
    "\n",
    "# Apply to data ;\n",
    "data_features = data_features[ind]\n",
    "data_labels = data_labels[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séparation  dataset into train / test sets: the first 80% are assigned to the train test, the other 20% to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate train set and test set\n",
    "# n_train : number of samples for train test\n",
    "n_train = ...\n",
    "\n",
    "x_train, x_test = data_features[0:n_train], data_features[n_train:-1]\n",
    "y_train, y_test = data_labels[0:n_train], data_labels[n_train:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure du réseau\n",
    "Le modèle est défini par ajout successif des couches. On peut commencer par une couche de normalisation si cela n'a pas été fait avant. Ensuite, on rajoute les couches successives en spécifiant le nombre de neurones et la fonction d'activation, typiquement  tanh, sigmoid ou relu. Etant donné qu'on est en classification binaire, la couche de sortie possède 1 seul neurone et la fonction d'activation est sigmoid.\n",
    "\n",
    "model.add(Dense(N, activation='...'))  \n",
    "où : \n",
    "- N est le nombre de neurones dans la couche\n",
    "- activation spécifie le type de fonction d'activation  \n",
    "\n",
    "Pour la première couche, on rajoute \"input_shape=k\", où k est le nombre de neurones en entrée du réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Normalization\n",
    "\n",
    "layer_norm = Normalization()\n",
    "layer_norm.adapt(x_train)\n",
    "\n",
    "model = Sequential()\n",
    "# add layers\n",
    "# Couche de normalisation si la normalisation n'a pas été réalisée avant\n",
    "model.add(...)\n",
    "# Couches cachées\n",
    "model.add(Dense(..., activation=.., input_dim=...))\n",
    "model.add(...)\n",
    "...\n",
    "# Couche de sortie\n",
    "model.add(...))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration complète\n",
    "Une fois la structure définie, le modèle doit être complètement configuré en spécifiant la fonction de coût, la métrique et l'algorithme d'optimisation.  \n",
    "\n",
    "On spécifie donc :\n",
    "* la fonction de coût utilisée pour calculer l'erreur à rétropropager : loss='...' ; (ex ici : 'binary_crossentropy')\n",
    "* l'algorithme d'optimisation : optimizer='...' (ex : 'adam', 'adamax', 'rmsprop', 'sgd')\n",
    "* la liste des métriques évaluées à titre indicatif à chaque étape : metrics=['...'] (ex ici : 'accuracy', 'AUC') \n",
    "\n",
    "On peut ensuite vérifier avec summary() que le modèle est conforme à ce que l'on souhaite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation\n",
    "model.compile(...)\n",
    "\n",
    "# Check the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage\n",
    "L'apprentissage est réalisé avec la fonction fit(...)  \n",
    "On spécifie : \n",
    "* le tableau des données en entrée\n",
    "* le tableau des données souhaitées en sortie\n",
    "* le nombre d'époques epochs=...  (commencer avec une petite valeur)  \n",
    "\n",
    "Facultatif :\n",
    "* la proportion des données utilisées pour la validation validation_split=... (typiquement entre 0.1 et 0.4)\n",
    "* la taille des batch batch=... (taille des paquets utilisés pour l'apprentissage ; typiquement 16, 32, 64)\n",
    "* verbose= 0, 1 ou 2 en fonction de ce que l'on souhaite afficher au cours de l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning step\n",
    "history = model.fit(...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La fonction \"draw_history\" fournie dans utils.py permet de visualiser l'évolution de l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import draw_history\n",
    "\n",
    "draw_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "L'évaluation finale est réalisée sur l'échantillon test. La fonction evaluate(...) donne le score global. La fonction predict(...) donne la prédiction échantillon par échnantillon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(...)\n",
    "print(\"The cost function on the test set is %.3f. The rate of correct prediction is %.2f\"%(score[0], score[1]))\n",
    "\n",
    "\n",
    "output_predict = model.predict(...)\n",
    "for sample_predict, sample_true in zip(output_predict[0:5], y_test[0:5]):\n",
    "    print(sample_predict, sample_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice\n",
    "\n",
    "Votre travail consiste maintenant à modifier le réseau pour améliorer les résultats. Quelques pistes :\n",
    "* ajouter des couches cachées\n",
    "* modifier le nombre de neurones\n",
    "* implanter des fonctions de régularisation type dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de formes à l'aide d'un réseau de convolution\n",
    "\n",
    "> Author: Françoise Bouvet (IJCLab, CNRS)  \n",
    "> Email: <francoise.bouvet@ijclab.in2p3.fr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "2. [Préparation des données](#Préparation-des-données)\n",
    "3. [Structure du réseau](#Structure-du-réseau)\n",
    "4. [Apprentissage](#Apprentissage)\n",
    "5. [Evaluation](#Evaluation)\n",
    "6. [Exercice](#Exercice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "L'objectif de ce TP est d'améliorer la reconnaissance des 5 formes géométriques avec un réseau de convolution.  \n",
    "\n",
    "Dans un premier temps, votre travail consiste à générer un réseau de convolution basique et à le tester. Pour cela, vous devez remplacer les ... par le code adéquat. Vous pourrez ensuite le compléter pour essayer d'améliorer les résultats.  \n",
    "\n",
    "Les données sont dans le répertoire data-shape. Il contient un sous-répertoire \"train\" dans lequel sont regroupés les fichiers d'apprentissage et un sous-répertoire \"test\" dans lequel sont regroupés les fichiers tests. Cinq formes différentes sont à distinguer : carré, rectangle, cercle, ellipse et triangle. Les images sont représentées sur 2 niveaux de gris, 0 pour l'intérieur des formes et 255 à l'extérieur. Elles sont de taille 64*64 pixels.  \n",
    "\n",
    "Les images sont issues de la base de données de la plateforme [kaggle](https://www.kaggle.com/)  \n",
    "\n",
    "Pour vous aider, vous trouverez des informations complémentaires sur la librairie [keras](https://keras.io/getting_started/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données\n",
    "\n",
    "La fonction de lecture des fichiers est fournie dans utils.py . Elle retourne un tableau avec les données d'entrée et un avec celles de sortie. Les données d'entrée sont ensuite normalisées sur [0,1] et celles en sortie sont transformées en variables catégorielles. Pour finir, les données sont mélangées : on utilise la fonction shuffle de numpy sur la liste des indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import lecture_shape_1channel\n",
    "\n",
    "rep_data = \"../datasets/data_shape/\"\n",
    "lst_shape = ['circle', 'ellipse', 'rectangle', 'square', 'triangle']\n",
    "\n",
    "# Read input data and transform output into one hot encoding\n",
    "input_train_raw, output_train_raw = lecture_shape_1channel(rep_data + \"train/\", \"*.png\", lst_shape)\n",
    "\n",
    "if input_train_raw is None or not np.any(input_train_raw):\n",
    "    print(f'Aucun fichier {extension} trouvé dans {dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Normalize input data\n",
    "input_train = input_train_raw.astype('float32') / 255.\n",
    "# Transform output into one hot encoding\n",
    "output_train = to_categorical(output_train_raw)\n",
    "\n",
    "# Shuffle input data\n",
    "ind = np.arange(0, np.shape(input_train)[0])\n",
    "np.random.shuffle(ind)\n",
    "\n",
    "# Apply to data ; \n",
    "input_train = input_train[ind]\n",
    "output_train = output_train[ind]\n",
    "\n",
    "print(f'Il y a {input_train.shape[0]} échantillons')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure du réseau\n",
    "Le modèle est défini par ajout successif des couches. Les données en entrée sont sous forme de tableau (64, 64, 1). On rajoute successivement la couche de convolution et la couche de pooling. \n",
    "\n",
    "Pour la couche de convolution, spécifier : \n",
    " \n",
    "* le nombre de filtres\n",
    "* la taille du noyau de chaque filtre : kernel_size=... (typiquement (3, 3) ou (5, 5))\n",
    "* la fontion d'activation : activation='...' (typiquement 'relu')\n",
    "* le type de pagging : padding='...' (typiquement 'same')\n",
    "* pour la première couche de convolution, spécifier input_shape est indispensablen si on veut utiliser la fonction summmary() function ensuite\n",
    "\n",
    "Pour le pooling, ajouter une couche de type MaxPooling2D ou AveragePooling2D et spécifier \n",
    "* la taille du regroupement en x et y : pool_size='...' (typiquement (2,2)) \n",
    "* le pas de déplacement : stride='...' (facultatif, par défaut la taille du pooling) \n",
    "\n",
    "Les données de sortie de la dernière couche de convolution sont transférées dans un vecteur 1D, puis on connecte les couches du perceptron multicouche. Etant donné qu'on a choisi un encodage de type one-hot pour les données en sortie, le nombre de neurones dans la couche de sortie est le nombre de classes et la fonction d'activation est softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "\n",
    "# Network definition : add successive layers\n",
    "model = Sequential()\n",
    "\n",
    "# Convolution layers\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(64, 64, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Reshape input 1D vector\n",
    "model.add(Flatten())\n",
    "\n",
    "# Full connected layer (MLP)\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# # Output layer\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration complète\n",
    "Une fois la structure définie, le modèle doit être complètement configuré en spécifiant la fonction de coût, la métrique et l'algorithme d'optimisation.  \n",
    "On spécifie donc :\n",
    "* la fonction de coût utilisée pour calculer l'erreur à rétropropager : loss='...' ; (ex ici : 'categorical_crossentropy')\n",
    "* l'algorithme d'optimisation : optimizer='...' (ex : 'adam', 'adamax', 'rmsprop', 'sgd')\n",
    "* la liste des métriques évaluées à titre indicatif à chaque étape : metrics=['...'] (ex ici : 'categorical_accuracy') \n",
    "\n",
    "On peut ensuite vérifier avec summary() que le modèle est conforme à ce que l'on souhaite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Display the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apprentissage\n",
    "L'apprentissage est réalisé avec la fonction fit(...)  \n",
    "On spécifie : \n",
    "* le tableau des données en entrée\n",
    "* le tableau des données souhaitées en sortie\n",
    "* le nombre d'époques epochs=...  (commencer avec une petite valeur)  \n",
    "\n",
    "Facultatif :\n",
    "* la proportion des données utilisées pour la validation validation_split=... (typiquement entre 0.1 et 0.4)\n",
    "* la taille des batch batch=... (taille des paquets utilisés pour l'apprentissage ; typiquement 16, 32, 64)\n",
    "* les fonctions de callback\n",
    "* verbose= 0, 1 ou 2 en fonction de ce que l'on souhaite afficher au cours de l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(input_train, output_train,\n",
    "                    epochs=20,\n",
    "                    validation_split=0.2,\n",
    "                    batch_size=32,\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La fonction \"draw_history\" fournie dans utils.py permet de visualiser l'évolution de l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import draw_history\n",
    "\n",
    "draw_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Dans un premier temps, il faut **lire les données tests** et les mettre au bon format. Cette étape est similaire à l'étape de préparation des données. Seule différence, les données sont dans le répertoire \"test\" et non \"train\".\n",
    "On peut évaluer le score du modèle sur l'ensemble des données test, et on peut prédire la classe échantillon par échantillon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test data\n",
    "input_test, output_test = lecture_shape_1channel(rep_data + \"test/\", \"*.png\", lst_shape)\n",
    "# Normalize test data\n",
    "input_test = input_test.astype('float32') / 255.\n",
    "# Transform output into one hot encoding\n",
    "output_test = to_categorical(output_test)\n",
    "\n",
    "# Test set shuffle only aims to display samples from all the classes\n",
    "ind = np.arange(np.shape(input_test)[0])\n",
    "np.random.shuffle(ind)\n",
    "input_test = np.array(input_test)[ind]\n",
    "output_test =np.array(output_test)[ind]\n",
    "\n",
    "# Evaluate the model ; the two parameters are the input_test array and the output_test array\n",
    "sum_score = model.evaluate(input_test, output_test)\n",
    "print(\"Data test : loss %.3f accuracy %.3f\" % (sum_score[0], sum_score[1]))\n",
    "\n",
    "# Prediction ; the input parameter is the input_test array ; return value is the prediction array\n",
    "output_predict = model.predict(input_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affichage des résultats sous forme de mosaïque d'images \n",
    "On peut utiliser la fonction draw_multiple_images(input_test, output_test, output_predict, lst_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import draw_multiple_images\n",
    "\n",
    "# Display the test images and the predicted class\n",
    "n=min(40, np.shape(input_test)[0])\n",
    "nb_col = 8\n",
    "draw_multiple_images(input_test[0:n], output_test[0:n], output_predict[0:n], lst_shape, nb_col, np.shape(input_test)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

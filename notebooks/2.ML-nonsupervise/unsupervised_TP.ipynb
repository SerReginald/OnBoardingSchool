{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "L'objectif de ce TP est de tester et de comprendre les différentes méthodes de ML non supervisé. Dans ce TP, vous devez remplacer les ... par le code adéquat. La seed aléatoire est fixée pour les différent TP, après les avoir complétés n'hésitez pas à la changer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Dimensionnality reduction](#Dimensionality-reduction)\n",
    "  - [PCA](#PCA)\n",
    "  - [Manifold Learning](#Manifold-Learning)\n",
    "- [Clustering](#Clustering)\n",
    "  - [KMeans](#KMeans)\n",
    "  - [DBSCAN](#DBSCAN)\n",
    "  - [GMM](#GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Réduction de dimension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Création d'un jeu de donné"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour commencer nous allons créer un jeu de donné constitué de points 2D corrélés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation d'un jeu de donnees avec deux axes corrélés\n",
    "np.random.seed(42)\n",
    "X = (np.random.rand(2, 2) @ np.random.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1]);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PCA\n",
    "**Principal Component Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour commencer nous allons utiliser l'analyse en composante principale pour décomposer ces données. Plus d'informations sur la PCA de sklearn sont disponibles sur leur site internet : https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un PCA avec 2 composantes et l'utiliser pour fitter X\n",
    "# Vous pouvez ensuite accéder aux composantes principales avec pca.components_ et à la variance avec pca.explained_variance_\n",
    "pca = ...\n",
    "pca.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Composantes principales : \")\n",
    "print(pca.components_)\n",
    "\n",
    "print(\"Variance : \")\n",
    "print(pca.explained_variance_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Visualisation des composantes\n",
    "\n",
    "En utilisant la fonction ci-dessous on peut visualiser les axes extraits par la PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "    arrowprops = dict(arrowstyle='->', color='k', lw=2, shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# A l'aide la la fonction draw_vector, ont peux les composantes principales sur le plot précédent\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Transformation des données\n",
    "\n",
    "On peut maintenant utiliser la PCA pour transformer les données selon les deux axes obtenus grâce à la méthode `fit_transform`. Les données résultantes doivent maintenant être décorrélées. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utiliser le PCA pour transformer X selon les composantes principales\n",
    "...\n",
    "X_trans = ...\n",
    "plt.scatter(X_trans[:, 0], X_trans[:, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Réduction de dimension\n",
    "\n",
    "On peut aussi utiliser la PCA pour réduire la dimension de nos données. Pour cela effectuer une PCA avec moins de dimensions que nos données d'entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En utilisant le PCA avec une dimension inférieur à celle de X, on peut réduire la dimension des données d'entrée\n",
    "...\n",
    "pca = ...\n",
    "X_trans = ...\n",
    "#Utiliser ensuite la méthode inverse_transform permet pour revenir dans l'espace d'origine\n",
    "...\n",
    "X2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], label='données initiales')\n",
    "plt.scatter(X2[:, 0], X2[:, 1], label='données après PCA')\n",
    "plt.legend(frameon=False, loc=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant essayer de réduire la dimension d'images de chiffres manuscrits à l'aide du PCA et de manifold learning.\n",
    "Les images sont accessibles grâce à digit.images (matrice 8x8), une version linéarisée de ces images peut être obtenue avec digits.data (vecteur de 64 éléments).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(10, 6), subplot_kw={'xticks': [], 'yticks': []})\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary_r')\n",
    "plt.grid(False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### PCA\n",
    "\n",
    "Pour commencer utiliser le PCA pour réduire la dimension des données à 2 composantes. On pourra ainsi visualiser la séparation des chiffres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_data = ...\n",
    "\n",
    "plt.figure(figsize=(10,6));\n",
    "plt.scatter(projected_data[:, 0], projected_data[:, 1], c=digits.target, edgecolor='none', alpha=0.5, cmap=plt.colormaps['cubehelix'])\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### t-SNE\n",
    "\n",
    "Comme nous l'avons vu en cours, le PCA ne fonctionne que pour les données séparables linéairement. Nous allons donc essayer le t-distributed stochastic neighbor embedding pour séparer les chiffres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser le TSNE pour réduire la dimension des données, la syntaxe est similaire à celle du PCA\n",
    "\n",
    "projected_data = ...\n",
    "plt.scatter(projected_data[:, 0], projected_data[:, 1], c=digits.target, edgecolor='none', alpha=0.5, cmap=plt.cm.get_cmap('cubehelix', 10))\n",
    "plt.colorbar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering\n",
    "\n",
    "Nous allons maintenant tester les techniques de clustering. Notre objectif est maintenant de rassembler ensemble les données similaires sans information a priori."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- K-means, DBSCAN\n",
    "\n",
    "    https://scikit-learn.org/stable/modules/clustering.html\n",
    "    \n",
    "- Gaussian mixture\n",
    "\n",
    "    https://scikit-learn.org/stable/modules/mixture.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Donnée\n",
    "\n",
    "Comme donnée pour tester les différents algorithmes de clustering, nous allons utiliser la fonction `make_blobs` de sklearn qui génère des paquets donnés selon des distributions gaussiennes. La fonction plot blob qui prend en entrée les données, leur label et le nombre de clusters nous permettent de les visualiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "np.random.seed(44)\n",
    "X, labels = make_blobs(n_samples=100, centers=10, cluster_std=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_blobs(X, y=None, n=3):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    if y is None:\n",
    "        ax.scatter(X[:, 0], X[:, 1])\n",
    "        ax.set_title(\"Raw data\", fontsize=14)\n",
    "    else:\n",
    "        im = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', n))\n",
    "        ax.set_title(\"Labeled data\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_blobs(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_blobs(X, labels, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### KMeans\n",
    "\n",
    "Commençons par utiliser la méthode du kmean. Toutes les méthodes de clustering de sklearn s'implémentent de la même manière. On définit les paramètres de notre modèle (ici avec `KMeans(...)`), on l'utilise pour fiter nos données avec `.fit(...)`, puis on accède au label de nos points à l'aide de `.predict(...)`.\n",
    "Lors de la création du kmean 3 paramètres sont importants : `n_clusters` le nombre de clusters, `init` le type de méthode d'initialisation utilisé pour choisir nos points de départ et `n_init` le nombre d'initialisations différentes qui vont être testées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisez KMeans pour prédire les labels des données X\n",
    "\n",
    "kmeans = KMeans(...).fit(...)\n",
    "y_kmeans = kmeans.predict(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_blobs(...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DBSCAN\n",
    "\n",
    "Utilisons maintenant DBScan avec nos données. L'interface est la même à la différence que l'on à pas besoin de préciser le nombre de clusters. Les deux paramètres importants lors de la création du DBScan sont : `eps` la distance entre deux points d'un même cluster et `min_samples` le nombre minimum de points nécessaire pour créer un cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Utiliser maintenant DBSCAN pour prédire les labels des données X \n",
    "dbscan = ...\n",
    "y_dbscan = ...\n",
    "\n",
    "print(f\"n_clusters = {len(set(dbscan.labels_))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Puis plotter le résultat\n",
    "plot_blobs(...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## GMM\n",
    "\n",
    "Pour finir, testons les GMM avec nos données de base. Il s'implémente exactement comme le kmean à la différence que le ne parle plus de cluster mais de composant `n_components`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour finir essayer la même chose avec un GaussianMixture\n",
    "\n",
    "gmm = ...\n",
    "y_gmm = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_blobs(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application à des jeux de données plus complexe\n",
    "\n",
    "Les données utilisées jusqu'ici étaient extrêmement simples, il est donc normal qu'on trouve les bons clusters.\n",
    "Essayons maintenant avec trois jeux de données plus complexes. À vous de déterminer la solution la plus appropriée à chacun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blob avec des corrélations\n",
    "np.random.seed(40)\n",
    "Xi, labelsi = make_blobs(n_samples=300, centers=5, cluster_std=1.5)\n",
    "X2 = Xi @ np.random.rand(2, 2)\n",
    "plot_blobs(X2, labelsi, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de blob inconue\n",
    "np.random.seed(111)\n",
    "X3, labels3 = make_blobs(n_samples=200, centers=int(10+np.random.rand()*20), cluster_std=0.3)\n",
    "plot_blobs(X3, labels3, labels3.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blobs avec des variances différentes supposées\n",
    "np.random.seed(44)\n",
    "Xb, labelsb = make_blobs(n_samples=300, centers=3, cluster_std=2)\n",
    "# Ajout de blobs sulémentaires\n",
    "X4 = np.vstack([X, Xb])\n",
    "labels4 = np.hstack([labels, labelsb+labels.max()+1])\n",
    "plot_blobs(X4, labels4, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

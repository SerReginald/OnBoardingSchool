{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "L'objectif de ce TP ce TP est de tester et de comprendre les différentes méthodes de ML non-supervisé. \n",
    "Dans ce TP, vous devez remplacer les ... par le code adéquat."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Dimensionnality reduction](#Dimensionality-reduction)\n",
    "  - [PCA](#PCA)\n",
    "  - [Manifold Learning](#Manifold-Learning)\n",
    "- [Clustering](#Clustering)\n",
    "  - [KMeans](#KMeans)\n",
    "  - [DBSCAN](#DBSCAN)\n",
    "  - [GMM](#GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Réduction de dimension"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Creation d'un jeu de donnné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation d'un jeu de donnees avec deux axes corrélés\n",
    "np.random.seed(42)\n",
    "X = (np.random.rand(2, 2) @ np.random.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1]);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PCA\n",
    "**Principal Component Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crée un PCA avec 2 composantes et l'utilise pour fitter X\n",
    "# Vous pouvez ensuite accéder aux composantes principales avec pca.components_ et à la variance avec pca.explained_variance_\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Composantes principales : \")\n",
    "print(pca.components_)\n",
    "\n",
    "print(\"Variance : \")\n",
    "print(pca.explained_variance_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Visualisation des composants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "    arrowprops = dict(arrowstyle='->', color='k', lw=2, shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# A l'aide la la fonction draw_vector, ont peux les composantes principales sur le plot précédent\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Transformation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Utiliser le PCA pour transformer X selon les composantes principales\n",
    "X_trans = pca.fit_transform(X)\n",
    "plt.scatter(X_trans[:, 0], X_trans[:, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Réduction de dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# En utilisant le PCA avec une dimension inférieur à celle de X, on peut réduire la dimension des données d'entrée\n",
    "pca = PCA(n_components=1)\n",
    "X_trans = pca.fit_transform(X)\n",
    "#Utiliser ensuite la fonction inverse_transform permet pour revenir dans l'espace d'origine\n",
    "X2 = pca.inverse_transform(X_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], label='données initiales')\n",
    "plt.scatter(X2[:, 0], X2[:, 1], label='données après PCA')\n",
    "plt.legend(frameon=False, loc=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant essayer de réduire la dimension de données de chiffres manuscrits à l'aide du PCA et de manifolfd learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(10, 6), subplot_kw={'xticks': [], 'yticks': []})\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary_r')\n",
    "plt.grid(False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour commencer utiliser le PCA pour réduire la dimension des données (digits.data)\n",
    "\n",
    "projected_data = PCA(2).fit_transform(digits.data)\n",
    "\n",
    "plt.figure(figsize=(10,6));\n",
    "plt.scatter(projected_data[:, 0], projected_data[:, 1], c=digits.target, edgecolor='none', alpha=0.5, cmap=plt.colormaps['cubehelix'])\n",
    "plt.colorbar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### t-SNE\n",
    "Comme nous l'avons vu en cours le PCA ne fonction que pour les donnée séparrable linéairent. Nous allons donc essayer le t-distributed stochastic neighbor embedding pour séparer les chiffres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Utiliser le TSNE pour réduire la dimension des données, la syntaxe est similaire à celle du PCA\n",
    "\n",
    "projected_data = TSNE(n_components=2).fit_transform(digits.data)\n",
    "plt.scatter(projected_data[:, 0], projected_data[:, 1], c=digits.target, edgecolor='none', alpha=0.5, cmap=plt.cm.get_cmap('cubehelix', 10))\n",
    "plt.colorbar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- K-means, DBSCAN\n",
    "\n",
    "    https://scikit-learn.org/stable/modules/clustering.html\n",
    "    \n",
    "- Gaussian mixture\n",
    "\n",
    "    https://scikit-learn.org/stable/modules/mixture.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "np.random.seed(44)\n",
    "X, labels = make_blobs(n_samples=100, centers=10, cluster_std=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_blobs(X, y=None, n=3):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    if y is None:\n",
    "        ax.scatter(X[:, 0], X[:, 1])\n",
    "        ax.set_title(\"Raw data\", fontsize=14)\n",
    "    else:\n",
    "        im = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.get_cmap('viridis', n))\n",
    "        ax.set_title(\"Labeled data\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_blobs(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_blobs(X, labels, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Utiliser KMeans pour prédire les labels des données X\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, init=\"k-means++\", n_init=5).fit(X)\n",
    "y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_blobs(X, y_kmeans, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utiliser maintenant DBSCAN pour prédire les labels des données X \n",
    "dbscan = DBSCAN(eps=0.8, min_samples=3)\n",
    "y_dbscan = dbscan.fit_predict(X)\n",
    "\n",
    "print(f\"n_clusters = {len(set(dbscan.labels_))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puis plotter le résultat\n",
    "plot_blobs(X, y_dbscan, len(set(dbscan.labels_)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## GMM\n",
    "**Gaussian Mixture Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Pour finir essayer la même chose avec un GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=10).fit(X)\n",
    "y_gmm = gmm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_blobs(X, y_gmm, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application à des jeux de donnée plus complexe\n",
    "\n",
    "Les donnée utilisé jusque ici étais extrement simple, il est donc normal qu'on trouve les bons clusters.\n",
    "Essayons maintenant avec trois jeux de données plus complexes. A vous de determiner la solution la plus approprié à chaque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blob avec des corrélations\n",
    "np.random.seed(40)\n",
    "Xi, labelsi = make_blobs(n_samples=300, centers=5, cluster_std=1.5)\n",
    "X2 = Xi @ np.random.rand(2, 2)\n",
    "plot_blobs(X2, labelsi, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm2 = GaussianMixture(n_components=5).fit(X2)\n",
    "y_gmm2 = gmm2.predict(X3)\n",
    "plot_blobs(X2, y_gmm2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de blob inconue\n",
    "np.random.seed(22)\n",
    "X3, labels3 = make_blobs(n_samples=200, centers=int(10+np.random.rand()*20), cluster_std=0.3)\n",
    "plot_blobs(X3, labels3, labels3.max()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"n_clusters original = {labels3.max()+1}\")\n",
    "\n",
    "#Utiliser maintenant DBSCAN pour prédire les labels des données X \n",
    "dbscan2 = DBSCAN(eps=0.4, min_samples=5)\n",
    "y_dbscan2 = dbscan3.fit_predict(X4)\n",
    "\n",
    "print(f\"n_clusters = {len(set(dbscan2.labels_))-1}\")\n",
    "\n",
    "plot_blobs(X3, y_dbscan2, len(set(dbscan2.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blobs avec des variances différentes supposées\n",
    "np.random.seed(10)\n",
    "Xb, labelsb = make_blobs(n_samples=300, centers=3, cluster_std=5)\n",
    "# Ajout de blobs sulémentaires\n",
    "X4 = np.vstack([X, Xb])\n",
    "labels4 = np.hstack([labels, labelsb+labels.max()+1])\n",
    "plot_blobs(X4, labels4, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN pour extraire les plus petit blobs\n",
    "dbscan3 = DBSCAN(eps=0.5, min_samples=5)\n",
    "y_dbscan3 = dbscan3.fit_predict(X4)\n",
    "\n",
    "print(f\"n_clusters = {len(set(dbscan3.labels_))-1}\")\n",
    "plot_blobs(X4, y_dbscan3, len(set(dbscan3.labels_)))\n",
    "\n",
    "# Le bruit contient les trois plus grand blobs\n",
    "X_noise = X4[y_dbscan3==-1]\n",
    "\n",
    "gmm3 = GaussianMixture(n_components=3).fit(X_noise)\n",
    "y_gmm3 = gmm3.predict(X_noise)\n",
    "plot_blobs(X_noise, y_gmm3, 3)\n",
    "\n",
    "# On remplace les labels du bruit par ceux du GMM\n",
    "y_dbscan3[y_dbscan3==-1] = y_gmm3\n",
    "\n",
    "plot_blobs(X4, y_dbscan3, len(set(y_dbscan3)))\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
